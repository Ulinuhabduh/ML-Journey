{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Languange Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1: Apa itu Teks? (The Raw Material)\n",
    "\n",
    "Di level paling dasar, komputer tidak melihat \"kata\" atau \"kalimat\". Ia hanya melihat serangkaian karakter: \n",
    "['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!'].\n",
    "\n",
    "\n",
    "- **Tantangan Utama NLP**: Bagaimana kita mengubah urutan karakter ini menjadi sesuatu yang memiliki makna dan bisa diolah secara matematis?\n",
    "\n",
    "- **Konsep Kunci:**\n",
    "  - **Corpus (jamak: Corpora)**: Kumpulan besar dari dokumen teks. Contoh: semua artikel Wikipedia, atau semua ulasan film di IMDb. Ini adalah \"bahan baku\" kita.\n",
    "  - **Document**: Satu unit teks dalam corpus. Contoh: satu artikel Wikipedia, atau satu ulasan film.\n",
    "  - **Token**: Unit dasar dari teks setelah diproses. Biasanya, sebuah \"kata\", tapi bisa juga tanda baca. Contoh: [\"Hello\", \",\", \"world\", \"!\"].\n",
    "  - **Vocabulary**: Kumpulan semua token unik yang ada di dalam corpus kita."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2: Membuat Teks Menjadi Terstruktur (Preprocessing)\n",
    "\n",
    "Sebelum kita bisa mengubah teks menjadi angka, kita harus membersihkan dan menstandarisasinya. Ini adalah tahap \"pencucian\" dan \"pemotongan\" bahan baku.\n",
    "\n",
    "- **Tujuan**: Mengurangi noise dan kompleksitas agar model bisa fokus pada sinyal yang penting.\n",
    "- **Teknik Umum (Pipeline Preprocessing)**:\n",
    "  - **Lowercasing**: Mengubah semua teks menjadi huruf kecil. \"Hello\" dan \"hello\" dianggap sama. Ini mengurangi ukuran vocabulary.\n",
    "  - **Tokenization**: Memecah kalimat menjadi token-token (kata). \"Hello world!\" menjadi ['Hello', 'world', '!']. Ini adalah langkah paling fundamental.\n",
    "  - **Removing Punctuation & Special Characters**: Menghapus tanda baca seperti ,, . atau karakter seperti #, @.\n",
    "  - **Removing Stopwords**: Menghapus kata-kata yang sangat umum tetapi seringkali tidak membawa banyak makna, seperti \"dan\", \"di\", \"yang\", \"a\", \"the\", \"is\".\n",
    "  - **Stemming & Lemmatization**: Mengurangi kata ke bentuk dasarnya.\n",
    "    - **Stemming**: Proses kasar memotong akhir kata. Contoh: computing, computer, computes -> comput. Cepat tapi kadang hasilnya bukan kata yang valid.\n",
    "    - **Lemmatization**: Proses yang lebih canggih menggunakan kamus untuk mengubah kata ke bentuk dasarnya (lemma). Contoh: am, are, is -> be; better -> good. Lebih lambat tapi lebih akurat.\n",
    "\n",
    "**Contoh Pipeline:**\n",
    "\n",
    "- **Input**: \"Wow, the computers are learning so fast!\"\n",
    "- **Lowercase**: \"wow, the computers are learning so fast!\"\n",
    "- **Tokenize**: ['wow', ',', 'the', 'computers', 'are', 'learning', 'so', 'fast', '!']\n",
    "- **Remove Punctuation**: ['wow', 'the', 'computers', 'are', 'learning', 'so', 'fast']\n",
    "- **Remove Stopwords**: ['wow', 'computers', 'learning', 'fast']\n",
    "- **Lemmatize**: ['wow', 'computer', 'learn', 'fast']\n",
    "- **Output Siap Proses**: ['wow', 'computer', 'learn', 'fast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mulin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mulin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mulin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mulin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Unduh paket-paket yang diperlukan (hanya perlu sekali)\n",
    "nltk.download('punkt')      # Untuk tokenisasi\n",
    "nltk.download('stopwords')  # Untuk daftar stopwords\n",
    "nltk.download('wordnet')    # Untuk lemmatisasi\n",
    "nltk.download('omw-1.4')    # Paket tambahan untuk WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks Mentah:\n",
      "'Wow, the computers at https://example.com are learning so fast! @ML_Lover #AI'\n",
      "\n",
      "Token Bersih:\n",
      "['wow', 'computer', 'learning', 'fast']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Membersihkan dan memproses satu string teks.\n",
    "    \n",
    "    Langkah-langkah:\n",
    "    1. Mengubah ke huruf kecil\n",
    "    2. Menghapus URL, handle @, dan karakter non-alfanumerik\n",
    "    3. Tokenisasi\n",
    "    4. Menghapus stopwords\n",
    "    5. Lemmatisasi\n",
    "    \n",
    "    Args:\n",
    "        text (str): String teks mentah.\n",
    "        \n",
    "    Returns:\n",
    "        list: Daftar token yang sudah dibersihkan.\n",
    "    \"\"\"\n",
    "    # 1. Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Menghapus elemen-elemen spesifik (URL, handle, hashtag, dll.)\n",
    "    # Hapus URL\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    # Hapus handle @ dan hashtag #\n",
    "    text = re.sub(r'@[^\\s]+|#[^\\s]+', '', text)\n",
    "    # Hapus karakter non-alfabet (menyisakan spasi)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 3. Tokenisasi\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 4. Menghapus stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 5. Lemmatisasi\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "# --- Contoh Penggunaan ---\n",
    "raw_text = \"Wow, the computers at https://example.com are learning so fast! @ML_Lover #AI\"\n",
    "cleaned_tokens = preprocess_text(raw_text)\n",
    "\n",
    "print(f\"Teks Mentah:\\n'{raw_text}'\")\n",
    "print(f\"\\nToken Bersih:\\n{cleaned_tokens}\")\n",
    "# Output yang diharapkan: ['wow', 'computer', 'learning', 'fast']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 3: Mengubah Teks Menjadi Angka (Vectorization/Feature Extraction)\n",
    "\n",
    "Di sini keajaiban dimulai. Kita mengubah daftar token yang bersih menjadi vektor numerik.\n",
    "\n",
    "- **Tujuan**: Merepresentasikan teks dalam format matematis yang bisa dipahami model ML.\n",
    "\n",
    "- **Metode Klasik (Berbasis Frekuensi):**\n",
    "  - **Bag-of-Words (BoW)**: Metode paling sederhana.\n",
    "    - **Cara kerja:** Bayangkan kita punya \"kantong\" (bag) berisi semua kata unik dari vocabulary. Untuk setiap dokumen, kita hitung berapa kali setiap kata dari vocabulary muncul di dokumen itu. Urutan kata diabaikan sama sekali.\n",
    "    - **Contoh**: Vocabulary: ['aku', 'suka', 'nasi', 'goreng']. Dokumen: \"aku suka nasi, aku suka goreng\". Vektor BoW-nya: [2, 2, 1, 1].\n",
    "    - **Kelemahan**: Mengabaikan urutan kata. \"aku tidak suka kamu\" dan \"kamu tidak suka aku\" akan memiliki vektor yang sama.\n",
    "  - **TF-IDF (Term Frequency-Inverse Document Frequency):** Peningkatan dari BoW.\n",
    "    - **Cara kerja:** Memberikan bobot pada setiap kata. Idenya adalah kata yang sering muncul di satu dokumen (tinggi TF) tetapi jarang muncul di semua dokumen lain dalam corpus (tinggi IDF) adalah kata yang penting dan spesifik untuk dokumen tersebut.\n",
    "    - **Contoh:** Dalam corpus artikel berita, kata \"presiden\" mungkin punya skor TF-IDF tinggi di artikel politik, tapi kata \"dan\" akan punya skor TF-IDF sangat rendah di semua artikel karena muncul di mana-mana.\n",
    "    - **Kelebihan:** Lebih baik dalam mengidentifikasi kata kunci yang informatif.\n",
    "  \n",
    "- **Metode Modern (Berbasis Konteks - Deep Learning):**\n",
    "  - **Word Embeddings (Contoh: Word2Vec, GloVe, FastText):**\n",
    "    - **Cara kerja:** Ini adalah revolusi dalam NLP. Setiap kata tidak lagi direpresentasikan oleh satu angka (seperti di BoW), tetapi oleh sebuah vektor padat (misal, 300 angka desimal). Vektor ini dipelajari oleh model neural network dari corpus besar.\n",
    "    - **Keajaiban:** Kata-kata dengan makna serupa akan memiliki vektor yang berdekatan secara matematis di ruang vektor. Kita bisa melakukan \"aljabar kata\", seperti: `vektor('Raja') - vektor('Pria') + vektor('Wanita') â‰ˆ vektor('Ratu')`.\n",
    "    - **Kelebihan:** Menangkap hubungan semantik dan sintaktis antar kata. Ini adalah fondasi dari hampir semua model NLP modern. Lapisan Embedding di Keras yang kita lihat kemarin melakukan hal ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bentuk matriks BoW: (5, 5)\n",
      "Ukuran vocabulary: 5\n",
      "\n",
      "Matriks Bag-of-Words (BoW):\n",
      " [[0 0 1 0 0]\n",
      " [0 0 2 0 1]\n",
      " [0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [1 1 1 1 0]]\n",
      "\n",
      "Fitur (Vocabulary):\n",
      " ['completely' 'different' 'document' 'new' 'second']\n",
      "\n",
      "\n",
      "--- Menjalankan ulang tanpa stopword removal untuk kejelasan ---\n",
      "\n",
      "Fitur (Vocabulary) tanpa stopword removal:\n",
      " ['and' 'completely' 'different' 'document' 'first' 'is' 'new' 'one'\n",
      " 'second' 'the' 'third' 'this']\n",
      "\n",
      "Matriks BoW tanpa stopword removal:\n",
      " [[0 0 0 1 1 1 0 0 0 1 0 1]\n",
      " [0 0 0 2 0 1 0 0 1 1 0 1]\n",
      " [1 0 0 0 0 1 0 1 0 1 1 1]\n",
      " [0 0 0 1 1 1 0 0 0 1 0 1]\n",
      " [1 1 1 1 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def vectorize_with_bow(corpus, max_features=5000):\n",
    "    \"\"\"\n",
    "    Mengubah sebuah corpus (daftar dokumen teks) menjadi matriks fitur Bag-of-Words (BoW).\n",
    "    \n",
    "    Args:\n",
    "        corpus (list of str): Daftar string, di mana setiap string adalah satu dokumen.\n",
    "        max_features (int): Batasi vocabulary dengan hanya mempertimbangkan 'max_features' kata\n",
    "                              paling umum. Ini sangat berguna untuk mengontrol dimensi.\n",
    "                              Jika None, semua fitur akan digunakan.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Berisi:\n",
    "            - sparse matrix: Matriks fitur BoW.\n",
    "            - CountVectorizer: Objek vectorizer yang sudah di-fit, bisa digunakan lagi untuk data baru.\n",
    "    \"\"\"\n",
    "    # Inisialisasi CountVectorizer.\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_features=max_features)\n",
    "    \n",
    "    # Fit vectorizer pada corpus dan transform corpus menjadi matriks BoW\n",
    "    X_bow = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    print(f\"Bentuk matriks BoW: {X_bow.shape}\")\n",
    "    print(f\"Ukuran vocabulary: {len(vectorizer.get_feature_names_out())}\")\n",
    "    \n",
    "    return X_bow, vectorizer\n",
    "\n",
    "# --- Contoh Penggunaan ---\n",
    "sample_corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "    \"A completely new and different document.\"\n",
    "]\n",
    "\n",
    "# Menggunakan fungsi untuk membuat matriks BoW\n",
    "bow_matrix, bow_vectorizer = vectorize_with_bow(sample_corpus, max_features=None) # Tidak membatasi fitur untuk contoh ini\n",
    "\n",
    "# Untuk melihat hasilnya (ini adalah sparse matrix, kita ubah ke array agar mudah dibaca)\n",
    "print(\"\\nMatriks Bag-of-Words (BoW):\\n\", bow_matrix.toarray())\n",
    "\n",
    "# Untuk melihat fitur (kata) apa yang diwakili oleh setiap kolom\n",
    "print(\"\\nFitur (Vocabulary):\\n\", bow_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Contoh interpretasi:\n",
    "# Baris pertama [0 1 1 0 0 0 1 0] sesuai dengan \"This is the first document.\"\n",
    "# Kata 'document' ada di indeks 1, 'first' di indeks 2, 'is' tidak ada (karena stopword), 'the' tidak ada (stopword), 'this' tidak ada (stopword).\n",
    "# Hmm, mari kita lihat fiturnya. Oh, vectorizer secara default men-lowercase semua.\n",
    "# 'document' ada di indeks 0, 'first' di 1, 'completely' di 2, dst.\n",
    "# Mari kita cek lagi baris pertama untuk \"this is the first document\" -> ['document', 'first']\n",
    "# 'document' (indeks 0) muncul 1x, 'first' (indeks 1) muncul 1x.\n",
    "# Jadi vektornya akan punya nilai 1 di kolom 0 dan 1.\n",
    "# Mari kita lihat: bow_matrix.toarray()[0]\n",
    "# Oh, `get_feature_names_out` sudah diurutkan. Mari kita lihat nama fitur dan posisinya.\n",
    "# 'document' -> kolom 1, 'first' -> kolom 2.\n",
    "# Vektor untuk baris pertama: [1, 1, 0, 0, 1, 0, 0] -> document=1, first=1, is=tidak ada, the=tidak ada, this=tidak ada.\n",
    "# Tunggu, kenapa ada 7 kolom? Mari kita lihat `get_feature_names_out()` -> ['completely', 'different', 'document', 'new', 'one', 'second', 'third']\n",
    "# Ah, 'first' dan 'this' dan 'is' terhapus oleh 'stop_words=english'.\n",
    "# Mari kita jalankan ulang tanpa stopwords untuk melihat perilakunya.\n",
    "\n",
    "print(\"\\n\\n--- Menjalankan ulang tanpa stopword removal untuk kejelasan ---\")\n",
    "vectorizer_no_stopwords = CountVectorizer(max_features=None)\n",
    "X_bow_no_stopwords = vectorizer_no_stopwords.fit_transform(sample_corpus)\n",
    "print(\"\\nFitur (Vocabulary) tanpa stopword removal:\\n\", vectorizer_no_stopwords.get_feature_names_out())\n",
    "print(\"\\nMatriks BoW tanpa stopword removal:\\n\", X_bow_no_stopwords.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bentuk matriks TF-IDF: (4, 2)\n",
      "Ukuran vocabulary: 2\n",
      "\n",
      "Matriks TF-IDF (sparse):\n",
      " [[1.         0.        ]\n",
      " [0.78722298 0.61666846]\n",
      " [0.         0.        ]\n",
      " [1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorize_with_tfidf(corpus):\n",
    "    \"\"\"\n",
    "    Mengubah sebuah corpus (daftar dokumen teks) menjadi matriks fitur TF-IDF.\n",
    "    \n",
    "    Args:\n",
    "        corpus (list of str): Daftar string, di mana setiap string adalah satu dokumen.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Berisi:\n",
    "            - sparse matrix: Matriks fitur TF-IDF.\n",
    "            - TfidfVectorizer: Objek vectorizer yang sudah di-fit, bisa digunakan lagi untuk data baru.\n",
    "    \"\"\"\n",
    "    # Inisialisasi vectorizer.\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "    \n",
    "    # Fit dan transform corpus\n",
    "    X_tfidf = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    print(f\"Bentuk matriks TF-IDF: {X_tfidf.shape}\")\n",
    "    print(f\"Ukuran vocabulary: {len(vectorizer.get_feature_names_out())}\")\n",
    "    \n",
    "    return X_tfidf, vectorizer\n",
    "\n",
    "# --- Contoh Penggunaan ---\n",
    "sample_corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "tfidf_matrix, tfidf_vectorizer = vectorize_with_tfidf(sample_corpus)\n",
    "\n",
    "# Untuk melihat hasilnya (ini adalah sparse matrix)\n",
    "print(\"\\nMatriks TF-IDF (sparse):\\n\", tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bentuk sekuens setelah padding: (4, 10)\n",
      "\n",
      "Matriks sekuens setelah padding:\n",
      " [[ 2  3  4  6  5  0  0  0  0  0]\n",
      " [ 2  5  3  4  7  5  0  0  0  0]\n",
      " [ 8  2  3  4  9 10  0  0  0  0]\n",
      " [ 3  2  4  6  5  0  0  0  0  0]]\n",
      "\n",
      "Word Index:\n",
      " {'<UNK>': 1, 'this': 2, 'is': 3, 'the': 4, 'document': 5, 'first': 6, 'second': 7, 'and': 8, 'third': 9, 'one': 10}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_for_deep_learning(corpus, vocab_size=10000, max_length=256):\n",
    "    \"\"\"\n",
    "    Mengubah corpus teks menjadi sekuens integer yang sudah di-padding, siap untuk model DL.\n",
    "    \n",
    "    Args:\n",
    "        corpus (list of str): Daftar dokumen teks.\n",
    "        vocab_size (int): Ukuran maksimum vocabulary yang akan digunakan.\n",
    "        max_length (int): Panjang maksimum setiap sekuens setelah padding.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Berisi:\n",
    "            - np.ndarray: Matriks sekuens yang sudah di-padding.\n",
    "            - Tokenizer: Objek tokenizer yang sudah di-fit.\n",
    "    \"\"\"\n",
    "    # Inisialisasi tokenizer\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<UNK>\")\n",
    "    \n",
    "    # Fit tokenizer pada corpus\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # Ubah teks menjadi sekuens integer\n",
    "    sequences = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    # Lakukan padding\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "    \n",
    "    print(f\"Bentuk sekuens setelah padding: {padded_sequences.shape}\")\n",
    "    \n",
    "    return padded_sequences, tokenizer\n",
    "\n",
    "# --- Contoh Penggunaan ---\n",
    "padded_matrix, keras_tokenizer = vectorize_for_deep_learning(sample_corpus, vocab_size=20, max_length=10)\n",
    "\n",
    "print(\"\\nMatriks sekuens setelah padding:\\n\", padded_matrix)\n",
    "\n",
    "# Untuk melihat word index yang dipelajari tokenizer\n",
    "print(\"\\nWord Index:\\n\", keras_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 4: Membangun Model (Tasks & Architectures)\n",
    "\n",
    "Setelah teks menjadi vektor, kita bisa memasukkannya ke model ML untuk melakukan berbagai tugas.\n",
    "\n",
    "**Tugas NLP Umum:**\n",
    "- **Klasifikasi Teks**: Memberi label pada sebuah dokumen. (Contoh: Klasifikasi Sentimen, Deteksi Spam, Kategorisasi Topik Berita).\n",
    "- **Named Entity Recognition (NER)**: Mengidentifikasi dan melabeli \"entitas\" dalam teks. (Contoh: \"Apple [ORG] didirikan oleh Steve Jobs [PER] di Cupertino [LOC]\").\n",
    "- **Machine Translation**: Menerjemahkan teks dari satu bahasa ke bahasa lain.\n",
    "- **Summarization**: Membuat ringkasan singkat dari teks yang panjang.\n",
    "- **Question Answering**: Memberikan jawaban berdasarkan konteks teks yang diberikan.\n",
    "\n",
    "**Arsitektur Model:**\n",
    "- **Model Klasik**: Naive Bayes, Logistic Regression, SVM (bekerja baik dengan fitur TF-IDF).\n",
    "- **Recurrent Neural Networks (RNN)**: Didesain khusus untuk data berurutan (seperti teks). Ia memiliki \"memori\" yang memproses kata satu per satu sambil mengingat apa yang telah ia lihat sebelumnya.\n",
    "    - **LSTM (Long Short-Term Memory) & GRU (Gated Recurrent Unit)**: Varian RNN yang jauh lebih baik dalam menangani \"memori jangka panjang\", mampu menghubungkan kata di awal kalimat dengan kata di akhir kalimat. Sangat populer untuk klasifikasi teks dan NER.\n",
    "- **Convolutional Neural Networks (CNN)**: Awalnya untuk gambar, tetapi juga bisa digunakan untuk NLP. Ia bekerja dengan melihat \"kelompok kata\" (n-grams) dan bisa sangat cepat dan efektif untuk klasifikasi.\n",
    "- **Transformers (Revolusi Terbaru)**: Arsitektur yang diperkenalkan di paper \"Attention Is All You Need\" (2017). Ia tidak memproses kata secara berurutan, melainkan melihat semua kata sekaligus dan menggunakan mekanisme \"Attention\" untuk menimbang seberapa penting setiap kata lain bagi kata yang sedang diproses.\n",
    "    - **Model-model seperti BERT, GPT (Generative Pre-trained Transformer), T5** adalah contoh model berbasis Transformer. Mereka adalah state-of-the-art untuk hampir semua tugas NLP saat ini. Mereka biasanya sudah di-pre-trained pada data teks yang masif (seluruh internet) dan bisa kita fine-tune untuk tugas spesifik kita.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
